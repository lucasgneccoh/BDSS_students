{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lucasgneccoh/BDSS_Dauphine/blob/main/notebooks/solutions/BDSS_TD3_SAX_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-9rovwlHP_X"
   },
   "source": [
    "# Bases de données semi-structurées - TD 3 - SAX\n",
    "\n",
    "Main teacher: **Dario COLAZZO**\n",
    "\n",
    "Teaching Assistant: **Lucas GNECCO**\n",
    "\n",
    "Special thanks to **Beatrice NAPOLITANO**\n",
    "\n",
    "Université Paris Dauphine - PSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook we will use the `xml.sax` module to read XML files. The idea of SAX is to treat the file in an event driven fashion so that it is very memory efficient. Imagine you are reading the file \"line by line\", where line does not mean exactly one line of text, but one \"XML line\" if that makes any sense.\n",
    "\n",
    "The basic events are *startElement*, to deal with the start of an element when an opening tag is found, *endElement* when we read an end tag, and *characters* when we read the text content of an element containing only text.\n",
    "\n",
    "This basic events and some clever Python coding will allow us to do some very cool stuff!\n",
    "\n",
    "Here is some important documentation and resources\n",
    "\n",
    "https://docs.python.org/3/library/xml.sax.html#module-xml.sax\n",
    "\n",
    "https://docs.python.org/3/library/xml.sax.handler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8i6vqq1FuQh"
   },
   "source": [
    "## Preambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SwLF3qxcFmqQ"
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from xml.dom.minidom import parse\n",
    "import xml.sax\n",
    "\n",
    "# Functions to work with XML files\n",
    "\n",
    "def validate_xml(xml_path:str, dtd_path:str) -> bool:\n",
    "    ''' Validate an XML file  against a DTD using the lxml library\n",
    "    '''\n",
    "    try:\n",
    "        dtd = etree.DTD(open(dtd_path))\n",
    "    except etree.DTDParseError as ed:\n",
    "        print(f\"DTDParseError: {ed}\")\n",
    "        for i, er in enumerate(ed.error_log):\n",
    "            print(f\"\\t{i}-> {er.message}, at line {er.line}\")\n",
    "        etree.clear_error_log()\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        xml_doc = etree.parse(xml_path)\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"XMLSyntaxError: {e}\")\n",
    "        for i, er in enumerate(e.error_log):\n",
    "            print(f\"\\t{i}-> {er.message}, at line {er.line}\")\n",
    "        etree.clear_error_log()\n",
    "        return False\n",
    "\n",
    "    result = dtd.validate(xml_doc)\n",
    "    if not result: print(dtd.error_log[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "def write_xml_dtd_files_from_strings(xml_strings, dtd_strings, identifiers = None):\n",
    "    ''' Write a list of strings into files. This strings should be XML and DTD files\n",
    "    '''\n",
    "\n",
    "    # If single strings are given, encapsulate them in lists  \n",
    "    if all(map(lambda o: isinstance(o, str), [xml_strings, dtd_strings])):\n",
    "        xml_strings, dtd_strings = [xml_strings], [dtd_strings]\n",
    "\n",
    "    if len(xml_strings) != len(dtd_strings):\n",
    "        raise Exception(\"Different number of XML and DTD strings!\")\n",
    "\n",
    "    # If no identifiers are given, create default ones. This determines file names\n",
    "    if identifiers is None:\n",
    "        identifiers = [f\"file_{i}\" for i in range(len(xml_strings))]\n",
    "\n",
    "    try:\n",
    "        for x, d, id in zip(xml_strings, dtd_strings, identifiers):\n",
    "            xml_path, dtd_path = f\"{id}.xml\", f\"{id}.dtd\" \n",
    "            with open(xml_path,\"w\") as f:\n",
    "                f.write(x)\n",
    "            with open(dtd_path,\"w\") as f:\n",
    "                f.write(d)\n",
    "    except Exception as e:\n",
    "        print(\"Problems while writing XML and DTD files\")\n",
    "        raise e\n",
    "\n",
    "    return identifiers\n",
    "\n",
    "\n",
    "\n",
    "def test_validation(xml_string, dtd_string, validator):\n",
    "    ''' Validate an XML document against a DTD, both given as strings\n",
    "    '''\n",
    "    # Write files\n",
    "    write_xml_dtd_files_from_strings(xml_string, dtd_string, identifiers = ['temp'])\n",
    "    \n",
    "    # Validate\n",
    "    return validator(\"temp.xml\", \"temp.dtd\" )\n",
    "\n",
    "def xpath_query_xml_string(xml_string, query_string):\n",
    "    xml_path = \"xml_doc.xml\"\n",
    "    with open(xml_path, \"w\") as f:\n",
    "        # Remove all whitespaces to keep the 'real' text of each node\n",
    "        f.write(re.sub(\">[\\s|\\n]*<\", \"><\", xml_string))\n",
    "        f.close()\n",
    "    xml_doc = etree.parse(xml_path)\n",
    "    query = etree.XPath(query_string)\n",
    "    return query(xml_doc)\n",
    "\n",
    "def xpath_query_xml_file(xml_path, query_string):\n",
    "    xml_doc = etree.parse(xml_path)\n",
    "    query = etree.XPath(query_string)\n",
    "    return query(xml_doc)\n",
    "\n",
    "\n",
    "def print_xpath_query_results(results):\n",
    "    print(f\"Total results: {len(results)}\")\n",
    "    print(\"*\"*20 + \"\\n\")\n",
    "    for e in results:\n",
    "        try:        \n",
    "            print(f\"node tag: {e.tag}\")\n",
    "            print(f\"node text: *{e.text}*\")\n",
    "            print(', '.join([f\"{k} = {v}\"for k, v in e.items()]))\n",
    "            print(\"-\"*20)\n",
    "        except:\n",
    "            print(\"--Except\")\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hge1MvnJx-r0"
   },
   "source": [
    "## SAX\n",
    "Download the FILMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-22 10:37:36--  https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/films.dtd\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 626 [text/plain]\n",
      "Saving to: ‘films.dtd’\n",
      "\n",
      "films.dtd           100%[===================>]     626  --.-KB/s    in 0s      \n",
      "\n",
      "2022-02-22 10:37:37 (17,1 MB/s) - ‘films.dtd’ saved [626/626]\n",
      "\n",
      "--2022-02-22 10:37:37--  https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/films.xml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43462 (42K) [text/plain]\n",
      "Saving to: ‘films.xml’\n",
      "\n",
      "films.xml           100%[===================>]  42,44K  --.-KB/s    in 0,004s  \n",
      "\n",
      "2022-02-22 10:37:37 (9,27 MB/s) - ‘films.xml’ saved [43462/43462]\n",
      "\n",
      "Files were downloaded correctly\n"
     ]
    }
   ],
   "source": [
    "dtd_link = \"https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/films.dtd\"\n",
    "xml_link = \"https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/films.xml\"\n",
    "\n",
    "!rm \"./films.dtd\"\n",
    "!rm \"./films.xml\"\n",
    "\n",
    "# Download the imdb sample file\n",
    "!wget {dtd_link}\n",
    "!wget {xml_link}\n",
    "\n",
    "# If the download fails, you will have to load the files into the Colab session. \n",
    "# Go to the Files section on the left panel\n",
    "\n",
    "if validate_xml(\"films.xml\", \"films.dtd\"):\n",
    "    print(\"Files were downloaded correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_U6dtDNyaDX"
   },
   "source": [
    "### Example 1: Print only certain elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIOY0to7yY4X",
    "outputId": "5f74a49d-2732-4e5a-94a0-36c105f6bc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4460>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1d7b5c4220>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n",
      "startElement: 'FILM'\n",
      "<xml.sax.xmlreader.AttributesImpl object at 0x7f1da00cce20>\n"
     ]
    }
   ],
   "source": [
    "class PrinterContentHandler(xml.sax.ContentHandler):\n",
    "    def __init__(self, tags_to_print = None):\n",
    "        super().__init__()\n",
    "        self.tags_to_print = tags_to_print\n",
    "        \n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if not self.tags_to_print is None and name in self.tags_to_print:\n",
    "            print(\"startElement: '\" + name + \"'\")\n",
    "            print(attrs)\n",
    "        return\n",
    "\n",
    "    def endElement(self, name):\n",
    "        return\n",
    "\n",
    "    def characters(self, content):\n",
    "        # When text is encountered\n",
    "        # print(\"Characters '\" + content + \"'\")\n",
    "        return\n",
    "\n",
    "\n",
    "tags_to_print = [\"FILM\"]\n",
    "handler = PrinterContentHandler(tags_to_print = tags_to_print)\n",
    "\n",
    "path = \"films.xml\"\n",
    "f = open(path)\n",
    "\n",
    "xml.sax.parse(f, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCtZp-qE8ooM"
   },
   "source": [
    "### Example 2: Get the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "PS4s356Q8mnf",
    "outputId": "2aa7b4aa-972d-4272-a817-3cd10caa5d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vertigo', 'Alien', 'Titanic', 'Sacrifice', 'Volte/Face', 'Sleepy Hollow', 'American Beauty', 'Impitoyable', 'Gladiator', 'Blade Runner', 'Piège de cristal', '58 minutes pour vivre', 'Van Gogh', 'Seven', \"L'armée des douze singes\", 'Le nom de la rose', 'Pulp fiction', 'Mary à tout prix', 'Terminator', 'Les dents de la mer', 'Le silence des agneaux', \"Le prince d'Egypte\", 'Godzilla', 'Matrix', 'Mission: Impossible', 'Kagemusha', 'Les pleins pouvoirs', 'Le gendarme et les extra-terrestres', 'Les frères pétards', 'Le monde perdu', 'Rain Man', 'Top Gun', 'Les bronzés font du ski', 'MICROCOSMOS', 'Psychose', 'Le retour du Jedi', 'Les oiseaux', 'Reservoir dogs', 'Eyes Wide Shut', 'Shining', 'Pas de printemps pour Marnie', 'Fenêtre sur cour', 'La mort aux trousses', \"Jeanne d'Arc\", 'Le cinquième élément', 'Léon', 'Nikita', 'Le grand bleu']\n"
     ]
    }
   ],
   "source": [
    "class GetTextInsideTag(xml.sax.ContentHandler):\n",
    "    def __init__(self, tag = None):\n",
    "        super().__init__()\n",
    "        self.tag = tag\n",
    "        self.reading = False\n",
    "        self.buffer = []\n",
    "        self.result = []\n",
    "        \n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if not self.tag is None and name == self.tag:\n",
    "            self.reading = True\n",
    "        return\n",
    "\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if self.buffer:\n",
    "            # If something was read, then add it to results\n",
    "            # before reseting the buffer\n",
    "            self.result.append(' '.join(self.buffer))\n",
    "        self.reading = False\n",
    "        self.buffer = []\n",
    "        return\n",
    "\n",
    "    def characters(self, content):\n",
    "        # When text is encountered\n",
    "        # print(\"Characters '\" + content + \"'\")\n",
    "        if self.reading: self.buffer.append(content)\n",
    "        return\n",
    "\n",
    "\n",
    "tag = \"TITRE\"\n",
    "handler = GetTextInsideTag(tag = tag)\n",
    "\n",
    "path = \"films.xml\"\n",
    "f = open(path)\n",
    "\n",
    "xml.sax.parse(f, handler)\n",
    "\n",
    "print(handler.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9IRkMvO98C_"
   },
   "source": [
    "### Example 3: Get the titles of films staring some artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nciaru1u-Lwi",
    "outputId": "23fda4e4-3c04-4e3e-8965-c5e810370688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Piège de cristal', '58 minutes pour vivre', \"L'armée des douze singes\", 'Pulp fiction', 'Le cinquième élément']\n"
     ]
    }
   ],
   "source": [
    "class GetFilmsByArtistInCast(xml.sax.ContentHandler):\n",
    "    def __init__(self, prenom, nom):\n",
    "        super().__init__()\n",
    "        self.prenomSearch = prenom\n",
    "        self.nomSearch = nom\n",
    "\n",
    "        self.result = []\n",
    "        self.titleBuffer = []\n",
    "        self.prenomBuffer = []\n",
    "        self.nomBuffer = []\n",
    "\n",
    "        self.titleTemp = None\n",
    "        self.prenomTemp = None\n",
    "        self.nomTemp = None\n",
    "        \n",
    "        self.whereInDoc = None\n",
    "\n",
    "        self.reading = False\n",
    "        \n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        self.whereInDoc = name\n",
    "        if name in [\"PRENOM\", \"NOM\", \"TITRE\"]:\n",
    "            self.reading = True\n",
    "        return\n",
    "\n",
    "    def endElement(self, name):\n",
    "        # If we read something that we need, then get the contentn and use it\n",
    "        if name == \"TITRE\":\n",
    "            self.titleTemp = ' '.join(self.titleBuffer)\n",
    "            self.titleBuffer = []\n",
    "        if name == \"PRENOM\":\n",
    "            self.prenomTemp = ' '.join(self.prenomBuffer)\n",
    "            self.prenomBuffer = []\n",
    "        if name == \"NOM\":\n",
    "            self.nomTemp = ' '.join(self.nomBuffer)\n",
    "            self.nomBuffer = []\n",
    "\n",
    "        # If we end reading a ROLE element, we can check if it contains the \n",
    "        # artist we want\n",
    "        if name == \"ROLE\":\n",
    "            if self.prenomSearch == self.prenomTemp and self.nomSearch == self.nomTemp:\n",
    "                self.result.append(self.titleTemp)\n",
    "        self.reading = False\n",
    "        return\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.reading:\n",
    "        # If we are in TITLE, we have to save it\n",
    "            if self.whereInDoc == \"TITRE\": self.titleBuffer.append(content)\n",
    "            if self.whereInDoc == \"PRENOM\": self.prenomBuffer.append(content)\n",
    "            if self.whereInDoc == \"NOM\": self.nomBuffer.append(content)\n",
    "        return\n",
    "\n",
    "    def endDocument(self):\n",
    "        self.titleBuffer = []\n",
    "        self.prenomBuffer = []\n",
    "        self.nomBuffer = []\n",
    "\n",
    "\n",
    "prenom, nom = \"Bruce\", \"Willis\"\n",
    "handler = GetFilmsByArtistInCast(prenom = prenom, nom = nom)\n",
    "\n",
    "path = \"films.xml\"\n",
    "f = open(path)\n",
    "\n",
    "xml.sax.parse(f, handler)\n",
    "\n",
    "print(handler.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eCeJkwzCk9F"
   },
   "source": [
    "### Ex 1: Queries with SAX on the movie dataset\n",
    "\n",
    "Try to do some other queries using SAX.\n",
    "Compare your results (and maybe even running times!) with other tools like XPath or DOM.\n",
    "\n",
    "I suggest the queries 1 to 8, and then query 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nNRy_rTKAeQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nhqhzsCKC37"
   },
   "source": [
    "## Data science with text\n",
    "\n",
    "Here I want to introduce you to some more advanced topics in Data Science and Machine Learning.\n",
    "\n",
    "We will use it as an excuse to practice SAX\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Bag of words and TF-IDF**\n",
    "\n",
    "In Data Science and Machine Learning, particularly in Natural Language Processing, the objects to study are text documents. There are different ways to study them, but from a mathematical perspective we need ways of encoding such text documents into more \"vectorial\" data\n",
    "\n",
    "The question becomes: ***How do you transform a piece of text into a vector to apply your algorithms on them?***\n",
    "\n",
    "\n",
    "One very common example is sentiment analysis. The basic idea is that you want to know if some text (for example a movie review or a tweet) is positive or negative towards a subject. This can be seen as a classification problem.\n",
    "\n",
    "\n",
    "The initial approach to turn a piece of text into a vector is the Bag of Words, where you characterize a document by the words that appear in it and their frequence. A more sophisticated approach can be TF-IDF that takes into account the number of words in each document and also the relative frequence of words across documents.\n",
    "\n",
    "See this site for a detailed and simple exmplanation if you have doubts\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "\n",
    "In this excercise we will try to create such document vectors for our movie dataset using SAX as a way to read the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCgCGY89Vf81"
   },
   "source": [
    "### Bag of words\n",
    "\n",
    "We need to go through the document, get the RESUME and build a vocabulary that contains all the words present in all the RESUMEs.\n",
    "\n",
    "We do not care about every word. We will remove unecessary words using a special library\n",
    "\n",
    "We are going to optimize our code and create the bag of words as we go through the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3O23avnS6Ll",
    "outputId": "8eb84c75-e0c1-4ee3-c33c-9b290c6f85b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pulp', 'Fiction', 'décrit', \"l'odyssée\", 'sanglante', 'burlesque', 'petits', 'malfrats', 'jungle', 'Hollywood,', \"s'entrecroisent\", 'destins', 'petits', 'tueurs,', \"d'un\", 'dangereux', 'gangster', 'marié', 'camée,', \"d'un\", 'boxeur', 'roublard,', 'prêteurs', 'gages', 'sadiques,', \"d'un\", 'caïd', 'élégant', 'dévoué,', \"d'un\", 'dealer', 'bon', 'mari', 'tourtereaux', 'gachette', 'facile...']\n",
      "['pulp', 'fiction', 'décrit', 'odyssée', 'sanglante', 'burlesque', 'petits', 'malfrats', 'jungle', 'hollywood', 'entrecroisent', 'destins', 'petits', 'tueurs', 'dangereux', 'gangster', 'marié', 'camée', 'boxeur', 'roublard', 'prêteurs', 'gages', 'sadiques', 'caïd', 'élégant', 'dévoué', 'dealer', 'bon', 'mari', 'tourtereaux', 'gachette', 'facile']\n",
      "Our final function\n",
      "['pulp', 'fiction', 'décrit', 'odyssée', 'sanglante', 'burlesque', 'petits', 'malfrats', 'jungle', 'hollywood', 'entrecroisent', 'destins', 'petits', 'tueurs', 'dangereux', 'gangster', 'marié', 'camée', 'boxeur', 'roublard', 'prêteurs', 'gages', 'sadiques', 'caïd', 'élégant', 'dévoué', 'dealer', 'bon', 'mari', 'tourtereaux', 'gachette', 'facile']\n"
     ]
    }
   ],
   "source": [
    "''' 2.1.1 \n",
    "Write a function that given some text, it eliminates all non important words\n",
    "and returns a list of words representing the text\n",
    "We will use a library called spacy for the stop words in french and we can use\n",
    "another library called gensim to help us do some other preprocessing\n",
    "\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "import gensim.parsing.preprocessing as prep\n",
    "import re\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), lambda x: re.sub('\\W+',' ',x) ,\\\n",
    "                    prep.strip_tags, prep.strip_punctuation, \\\n",
    "                    prep.strip_multiple_whitespaces, \\\n",
    "                    prep.strip_numeric, \\\n",
    "                    prep.strip_short]\n",
    "our_prep_func = lambda x: prep.preprocess_string(x, CUSTOM_FILTERS)\n",
    "\n",
    "\n",
    "# Traiter le texte\n",
    "resume = \"Pulp Fiction décrit l'odyssée sanglante et burlesque de petits malfrats \\\n",
    "dans la jungle de Hollywood, ou s'entrecroisent les destins de deux petits \\\n",
    "tueurs, d'un dangereux gangster marié à une camée, d'un boxeur roublard, de \\\n",
    "prêteurs sur gages sadiques, d'un caïd élégant et dévoué, d'un dealer bon \\\n",
    "mari et de deux tourtereaux à la gachette facile...\"\n",
    "\n",
    "# Lets test the two approaches to see the difference\n",
    "new = [s for s in resume.split() if s not in fr_stop]\n",
    "print(new)\n",
    "\n",
    "new = [s for s in our_prep_func(resume) if s not in fr_stop]\n",
    "print(new)\n",
    "\n",
    "\n",
    "# This is the actual function\n",
    "def process_text(text, processing, stopwords = fr_stop):\n",
    "    return [s for s in processing(text) if s not in stopwords]\n",
    "\n",
    "\n",
    "print(\"Our final function\")\n",
    "new = process_text(resume, our_prep_func, fr_stop)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgJolpfpcrIg",
    "outputId": "4c058ebc-661d-4c9c-bfb2-f94be2d46435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "''' 2.1.2\n",
    "Defining the word count of a document\n",
    "Write a function that given a list of words and a vocabulary, computes the word\n",
    "count representation of the text\n",
    "The vocabulary will be represented as a dictionary containing pairs (word, index)\n",
    "where index is the position of the word in the vocabulary\n",
    "\n",
    "We will suppose that all the words are in the vocabulary\n",
    "'''\n",
    "\n",
    "def bag_of_words(tokens, vocab):\n",
    "    vector = [0]*len(vocab)\n",
    "    for s in tokens:\n",
    "        vector[vocab[s]] += 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Test it with a simple example\n",
    "tokens = [\"test\", \"sentence\", \"test\", \"test\", \"horse\", \"sentence\"]\n",
    "vocab = {\"sentence\":0, \"horse\":1, \"test\":2}\n",
    "bow = bag_of_words(tokens, vocab)\n",
    "print(bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ip9A8zfwfkVl",
    "outputId": "a4e22c68-d594-45c1-e502-9f7411049914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 0, 'horse': 1, 'test': 2, 'new_word': 3, 'more_novelty': 4}\n"
     ]
    }
   ],
   "source": [
    "''' 2.1.3\n",
    "Expanding a vocabulary\n",
    "When we read new text, some new words might appear. We need to add them to the\n",
    "vocabulary we are considering.\n",
    "Write a function that given a vocabulary and some new text (already preprocessed),\n",
    "adds the new words to the vocabulary (if there are new words)\n",
    "'''\n",
    "\n",
    "def expand_vocab(vocab, new_tokens):\n",
    "    for s in new_tokens:\n",
    "        if not s in vocab:\n",
    "            vocab[s] = len(vocab)\n",
    "\n",
    "\n",
    "# Test\n",
    "tokens = [\"test\", \"new_word\", \"horse\", \"more_novelty\"]\n",
    "vocab = {\"sentence\":0, \"horse\":1, \"test\":2}\n",
    "expand_vocab(vocab, tokens)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOHrXZgZkICh",
    "outputId": "c83a07d1-5b2a-4fb6-eb90-189f52b60eca"
   },
   "outputs": [],
   "source": [
    "''' 2.1.4\n",
    "Put it all together\n",
    "Now use the functions we created and SAX to build the bag of words representation\n",
    "for all the resumes\n",
    "\n",
    "Note: We will take care of the length of the vectors later\n",
    "'''\n",
    "\n",
    "class ResumeBagOfWordsSax(xml.sax.ContentHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reading = False\n",
    "        self.vectors = {}\n",
    "        self.titleTemp = None\n",
    "        self.resumeTemp = None\n",
    "        self.buffer = []\n",
    "        self.vectorTemp = None\n",
    "        self.vocab = {}\n",
    "        self.tokensSave = {}\n",
    "        \n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        self.whereInDoc = name\n",
    "        if name in [\"TITRE\", \"RESUME\"]:\n",
    "            self.reading = True\n",
    "        return\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == \"TITRE\":\n",
    "            # We finished reading the title, so we create the entry in the dictionary\n",
    "            self.titleTemp = ' '.join(self.buffer)\n",
    "            # If the RESUME was before, then the vector is ready\n",
    "            if not self.vectorTemp is None:\n",
    "                self.vectors[self.titleTemp] = self.vectorTemp\n",
    "            else:\n",
    "                self.vectors[self.titleTemp] = []\n",
    "\n",
    "        if name == \"RESUME\":\n",
    "            # We finished reading a RESUME. We need to create the BoW\n",
    "            self.resumeTemp = ' '.join(self.buffer)\n",
    "            # Preprocess the text\n",
    "            tokens = process_text(self.resumeTemp, our_prep_func, fr_stop)\n",
    "            # Expand the vocabulary\n",
    "            expand_vocab(self.vocab, tokens)\n",
    "            # Create the BoW vector\n",
    "            bow = bag_of_words(tokens, self.vocab)\n",
    "            # Save it\n",
    "            if not self.titleTemp is None:\n",
    "                self.vectors[self.titleTemp] = bow\n",
    "                self.tokensSave[self.titleTemp] = tokens\n",
    "            else:\n",
    "                # We haven't read the title, so we stock the vector in the \n",
    "                # temporal variable\n",
    "                self.vectorTemp = bow\n",
    "        \n",
    "        if name == \"FILM\":\n",
    "            # Restart the temporal values\n",
    "            self.titleTemp = None\n",
    "            self.resumeTemp = None\n",
    "            self.vectorTemp = None\n",
    "        self.reading = False\n",
    "        self.buffer = []\n",
    "        return\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.reading:\n",
    "            self.buffer.append(content)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "handler = ResumeBagOfWordsSax()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-22 10:38:40--  https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/imdb_simple_example.xml\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1219 (1,2K) [text/plain]\r\n",
      "Saving to: ‘imdb_simple_example.xml.4’\r\n",
      "\r\n",
      "\r",
      "          imdb_simp   0%[                    ]       0  --.-KB/s               \r",
      "imdb_simple_example 100%[===================>]   1,19K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2022-02-22 10:38:40 (144 MB/s) - ‘imdb_simple_example.xml.4’ saved [1219/1219]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Download the dummy file\n",
    "!wget \"https://raw.githubusercontent.com/lucasgneccoh/BDSS_Dauphine/main/data/imdb_simple_example.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vertigo', [5, 2, 1, 4, 1, 1, 1, 1])\n",
      "('Alien', [0, 0, 0, 2, 2, 0, 0, 6, 1, 1, 2])\n",
      "('Titanic', [0, 0, 0, 4, 1, 0, 0, 3, 1, 0, 2, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"imdb_simple_example.xml\"\n",
    "f = open(path)\n",
    "\n",
    "xml.sax.parse(f, handler)\n",
    "\n",
    "print(*handler.vectors.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yktmPf0duqeB",
    "outputId": "9ce2c6fa-8599-4f76-de6e-8d8b029cab0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vectors after resize\n",
      "('Vertigo', [5, 2, 1, 4, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "('Alien', [0, 0, 0, 2, 2, 0, 0, 6, 1, 1, 2, 0, 0, 0, 0])\n",
      "('Titanic', [0, 0, 0, 4, 1, 0, 0, 3, 1, 0, 2, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Lets fill the vectors with zeros to get them to the right length\n",
    "'''\n",
    "N = len(handler.vocab)\n",
    "for k, v in handler.vectors.items():\n",
    "    v += [0]*(N-len(v))\n",
    "\n",
    "print(\"New vectors after resize\")\n",
    "print(*handler.vectors.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "kW-1IScsun3m",
    "outputId": "ebe31599-fbb8-422d-e654-b3ce79357c10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vertigo</th>\n",
       "      <th>Alien</th>\n",
       "      <th>Titanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointing</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Vertigo  Alien  Titanic\n",
       "bad                  5      0        0\n",
       "terrible             2      0        0\n",
       "horrible             1      0        0\n",
       "very                 4      2        4\n",
       "super                1      2        1\n",
       "disappointing        1      0        0\n",
       "not                  1      0        0\n",
       "good                 1      6        3\n",
       "perfect              0      1        1\n",
       "excellent            0      1        0\n",
       "loved                0      2        2\n",
       "great                0      0        2\n",
       "movie                0      0        1\n",
       "sad                  0      0        1\n",
       "but                  0      0        1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "We can create a table to better understand each vector\n",
    "\n",
    "NOTE: This only makes sense with dummy datasets\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(data = handler.vectors, index = handler.vocab.keys())\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt-Q_w01qgkO"
   },
   "source": [
    "### TF-IDF\n",
    "Now that we have the Bag of Words for each resume, we can create the TF-IDF representations\n",
    "\n",
    "This can be done in serveral ways, and you can try the one you think is more fun\n",
    "\n",
    "\n",
    "\n",
    "*   Use plain Python\n",
    "*   Use *numpy* (vectors, vector operations)\n",
    "*   Use *pandas* (easiest way IMO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rReIEPiTqf8R"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7L6BwuoK6AX67Wlu10ZkB",
   "collapsed_sections": [
    "V8i6vqq1FuQh",
    "hge1MvnJx-r0",
    "l_U6dtDNyaDX",
    "lCtZp-qE8ooM",
    "Z9IRkMvO98C_",
    "5nhqhzsCKC37"
   ],
   "include_colab_link": true,
   "name": "BDSS_2021_TD3_SAX_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
